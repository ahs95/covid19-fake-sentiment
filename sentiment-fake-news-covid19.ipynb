{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12932426,"sourceType":"datasetVersion","datasetId":8180840}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"86fc7862-1be9-45cf-9db7-17be3a54ea94","_cell_guid":"7a3632c5-1ee5-4055-a70a-80fd5859fa66","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/csebuetnlp/normalizer\n!pip install transformers openpyxl tqdm datasets scikit-learn gradio torch accelerate -q","metadata":{"_uuid":"790e5ddb-413a-4cdd-a029-646c3ccab810","_cell_guid":"550bba46-04de-4683-a852-47f65506dc00","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math, json, re, random, time, gc\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup, AutoConfig, PreTrainedModel, PretrainedConfig\n\nfrom normalizer import normalize\nfrom multiprocessing import Pool, cpu_count\nimport shutil\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"65dcc6e0-8875-4808-86b0-dba1b0da913b","_cell_guid":"7281041b-71c8-40ed-bed8-da5f1e58162b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True  # Faster convolutions for fixed input sizes\ntorch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 for matmul (faster)\ntorch.backends.cudnn.allow_tf32 = True  # Allow TF32 for convolutions","metadata":{"_uuid":"7062c431-19e4-46c9-8795-222eecfd1783","_cell_guid":"64ce1a09-e1e8-4851-9744-a775f892b77e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")","metadata":{"_uuid":"75fd1d44-676d-421f-a354-c71b0c82aff8","_cell_guid":"26b2efbe-a2dc-4fc1-be82-a4def25942c5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Paths -----\nEXCEL_PATH = \"/kaggle/input/data-covid-sa-fake-sentiment/data-covid-sa-fake-sentiment.xlsx\"  # change if needed\nSAVE_DIR   = \"/kaggle/working/banglabert_multitask\"\nos.makedirs(SAVE_DIR, exist_ok=True)","metadata":{"_uuid":"34fa46a0-5656-44ef-bb5b-4107577a8215","_cell_guid":"354f17ab-8f3c-40ca-9f79-23ee42607317","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Hyperparameters -----\nMODEL_NAME = \"csebuetnlp/banglabert\"\nMAX_LENGTH = 512\nBATCH_SIZE = 16\nGRAD_ACCUM_STEPS = 2\nEPOCHS = 15\nFIXED_LR = 2e-5\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\nALPHA_SENTIMENT = 0.85\nDEBUG_MODE = False\n\nUSE_GRAD_CHECKPOINT = False","metadata":{"_uuid":"0ee05b1a-17d6-49ac-8047-479b48bc826a","_cell_guid":"67e3e94d-9e21-4910-85da-8250c2d3361e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SENTIMENT_MAP = {\"negative\":0, \"neutral\":1, \"positive\":2}\nTRUTH_MAP     = {\"fake\":0, \"real\":1}","metadata":{"_uuid":"0665a744-ab91-46ba-89fe-e0156d296788","_cell_guid":"d5c89179-4390-4877-9826-794d2f7e3f0d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reverse maps for decoding\nidx2sent = {v: k for k, v in SENTIMENT_MAP.items()}\nidx2truth = {v: k for k, v in TRUTH_MAP.items()}","metadata":{"_uuid":"008c41bf-cc96-4b63-b840-39cf06313752","_cell_guid":"bb144e7b-9771-472c-8d43-9a54bffd8d06","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_excel(EXCEL_PATH)","metadata":{"_uuid":"53057c1d-81ec-464d-938d-e99c61eb2348","_cell_guid":"784d68c9-677f-4e30-a7a8-b336125ea525","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# keep first occurrence of exact duplicates (text + both labels identical)\ndf = df.drop_duplicates(subset=[\"text\", \"sentiment\", \"truthfulness\"], keep=\"first\")","metadata":{"_uuid":"c3dac023-d519-4544-af61-7dbe7246039b","_cell_guid":"f117d068-f26c-4d52-b34e-abf06693e639","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Basic Overview ---\nprint(\"Dataset shape:\", df.shape)\nprint(\"\\nColumn names:\", df.columns.tolist())\nprint(\"\\nData types:\\n\", df.dtypes)\nprint(\"\\nMissing values:\\n\", df.isnull().sum())\nprint(\"\\nSample rows:\\n\", df.head())","metadata":{"_uuid":"c930b4b0-3b8b-4f92-95d3-732656f5cee4","_cell_guid":"a424a247-b61d-40b0-b421-28f43fc9b063","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['truthfulness'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Sentiment Distribution ---\nplt.figure(figsize=(6,4))\nsns.countplot(x='sentiment', data=df, order=df['sentiment'].value_counts().index)\nplt.title(\"Sentiment Distribution\")\nplt.show()","metadata":{"_uuid":"91e93deb-1677-43d9-a702-add6ff456300","_cell_guid":"539e0bcb-f509-4d1d-b25a-fa7ead798ece","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Fake vs Real Distribution ---\nplt.figure(figsize=(6,4))\nsns.countplot(x='truthfulness', data=df)\nplt.title(\"Fake vs Real Posts\")\nplt.show()","metadata":{"_uuid":"c873da07-5be1-4f64-a4da-bc2238c8ee65","_cell_guid":"12990147-d2e2-4990-946f-836ec43ebdbc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crosstab of Fake/Real vs Sentiment\ncross_tab = pd.crosstab(df['sentiment'], df['truthfulness'])\ncross_tab.plot(kind='bar', stacked=True, figsize=(8,5), colormap='viridis')\nplt.title(\"Sentiment vs Truthfulness\")\nplt.show()","metadata":{"_uuid":"dc663dd9-d48d-41e2-b8c9-1eefcc962cb0","_cell_guid":"5ccaa36f-6d74-48da-aa8c-ac376610439d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Text Length Analysis ---\ndf['char_count'] = df['text'].apply(len)\ndf['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n\nplt.figure(figsize=(8,4))\nsns.histplot(df['word_count'], bins=30, kde=True)\nplt.title(\"Distribution of Post Word Counts\")\nplt.show()\n\nplt.figure(figsize=(8,4))\nsns.boxplot(x='sentiment', y='word_count', data=df)\nplt.title(\"Post Length by Sentiment\")\nplt.show()","metadata":{"_uuid":"5365b62b-30d6-4b79-917a-0729a36d41ee","_cell_guid":"d8fa8cdf-4939-4d63-b312-f731b5568d62","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 7. Heatmap: Sentiment vs Fake/Real ---\nplt.figure(figsize=(6,4))\nsns.heatmap(cross_tab, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Heatmap: Sentiment vs Fake/Real\")\nplt.show()","metadata":{"_uuid":"dcc3e08e-25f2-44f5-829e-adbadf9492cb","_cell_guid":"33a0bbfd-edeb-40f3-9015-8d2f1a11c438","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1Ô∏è‚É£ Compile the ‚Äúwhitespace‚Äëcollapse‚Äù regex once (global)\n_COLLAPSE_RE = re.compile(r\"\\s+\")\n\ndef _clean_one(text: str) -> str:\n    \"\"\"\n    Normalise a **single** string, then collapse whitespace and strip.\n    This function is deliberately tiny ‚Äì it will be executed in a separate\n    process, so we want to avoid any heavy imports inside it.\n    \"\"\"\n    txt = normalize(\n        text,\n        unicode_norm=\"NFKC\",\n        punct_replacement=None,\n        url_replacement=\"\",\n        emoji_replacement=\"\",\n        apply_unicode_norm_last=True,\n    )\n    # Collapse any run of whitespace into a single space\n    txt = _COLLAPSE_RE.sub(\" \", txt).strip()\n    return txt","metadata":{"_uuid":"c465df4d-b1d5-4f31-8e67-ae3b41719b8c","_cell_guid":"20c24b38-b55d-47be-8c44-3007d5a34ff2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _batch_normalize(texts):\n    \"\"\"\n    Wrapper that receives a *list* of strings (one chunk) and returns the\n    cleaned list.  It is executed in a worker process.\n    \"\"\"\n    return [_clean_one(t) for t in texts]","metadata":{"_uuid":"80f7900f-cae2-4b47-b487-6e433b216487","_cell_guid":"083cd1b5-4d7c-404f-aacc-80e2057131f4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parallel_preprocess(text_series, n_jobs: int = None, chunk_size: int = 2000):\n    \"\"\"\n    Fast multi‚Äëprocess preprocessing.\n\n    Parameters\n    ----------\n    text_series : pd.Series\n        Column that contains raw text.\n    n_jobs : int, optional\n        Number of worker processes ‚Äì default = half of the available CPUs.\n    chunk_size : int, optional\n        How many rows are sent to a worker at once.  Larger chunks ‚Üí less\n        inter‚Äëprocess communication overhead.\n\n    Returns\n    -------\n    pd.Series\n        Cleaned text, same length as the input.\n    \"\"\"\n    if n_jobs is None:\n        # Colab free tier typically has 2 vCPU ‚Üí use 1 or 2 processes.\n        n_jobs = max(1, cpu_count() // 2)\n\n    # Convert to a plain list of strings (ensures no NaNs)\n    raw_texts = text_series.astype(str).tolist()\n\n    # ------------------------------------------------------------------\n    # 2Ô∏è‚É£ Split the list into chunks (roughly `len(raw_texts) / n_jobs`)\n    # ------------------------------------------------------------------\n    total = len(raw_texts)\n    if total == 0:\n        return pd.Series([], dtype=str)\n\n    # Calculate optimal chunk size if not supplied\n    if chunk_size is None:\n        chunk_size = max(1, total // (n_jobs * 4))\n\n    # Build list of slices\n    chunks = [raw_texts[i : i + chunk_size] for i in range(0, total, chunk_size)]\n\n    # ------------------------------------------------------------------\n    # 3Ô∏è‚É£ Run the heavy work in parallel\n    # ------------------------------------------------------------------\n    print(f\"üîß Pre‚Äëprocessing {total:,} rows with {n_jobs} process(es) \"\n          f\"(chunk size ‚âà {chunk_size}) ‚Ä¶\")\n    with Pool(processes=n_jobs) as pool:\n        # `imap_unordered` yields results as soon as a worker finishes a chunk\n        cleaned_chunks = list(pool.imap_unordered(_batch_normalize, chunks, chunksize=1))\n\n    # ------------------------------------------------------------------\n    # 4Ô∏è‚É£ Flatten the list of chunks back into one list and wrap as Series\n    # ------------------------------------------------------------------\n    cleaned_texts = [txt for chunk in cleaned_chunks for txt in chunk]\n\n    # Verify length sanity (helps catching bugs early)\n    assert len(cleaned_texts) == total, (\n        f\"Length mismatch after cleaning: expected {total}, got {len(cleaned_texts)}\"\n    )\n    return pd.Series(cleaned_texts, index=text_series.index)","metadata":{"_uuid":"5c967d91-5ed0-497c-9549-19359851b01d","_cell_guid":"62b9a358-5461-44ec-90ba-a6eb18d17097","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load custom stop words from a file\ndef load_custom_stop_words(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        stop_words = set(file.read().splitlines())\n    return stop_words\n\nstop_words_file_path = '/kaggle/input/data-covid-sa-fake-sentiment/Stopwords.txt'\ncustom_stop_words = load_custom_stop_words(stop_words_file_path)","metadata":{"_uuid":"7c17ab09-0bfa-44df-91ea-f1a0ff0ee4a7","_cell_guid":"c2421375-27c6-4438-821e-2f3f659d096c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stopwords_removal(text):\n    text = ' '.join([word for word in text.split() if word.lower() not in custom_stop_words])\n    return text\n\ndf['text'] = df['text'].astype(str).apply(stopwords_removal)\ndf = df[df['text'].str.strip() != \"\"]","metadata":{"_uuid":"e5d93def-e0f2-4a80-b75f-e5d3d7ae2836","_cell_guid":"aa79a0b4-f09f-469b-b00c-d2e92adbf4c5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Usage\ndf[\"text\"] = parallel_preprocess(df[\"text\"])","metadata":{"_uuid":"c9891ffa-317f-49f6-b4a8-95eded09e4e0","_cell_guid":"5a24d476-35fc-471d-a972-5a61b3eeeca6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SENTIMENT_ALIASES = {\n    \"pos\":\"positive\", \"positive\":\"positive\",\n    \"neg\":\"negative\", \"negative\":\"negative\",\n    \"neu\":\"neutral\",  \"neutral\":\"neutral\",\n    \"Positive\":\"positive\", \"Neutral\":\"neutral\", \"Negative\":\"negative\"\n}\nTRUTH_ALIASES = {\n    \"fake\":\"fake\",\"fake news\":\"fake\",\"false\":\"fake\",\n    \"real\":\"real\",\"true\":\"real\",\"true news\":\"real\"\n}","metadata":{"_uuid":"b198dd85-07e8-4b92-8e0c-4290f6feb00e","_cell_guid":"688e09fa-5e73-4ba4-99ea-1b28cd261f0f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_series(series, label_map, aliases, colname):\n    \"\"\"\n    Label encoding with alias normalization.\n    - Lowercases + strips values\n    - Replaces using alias dictionary\n    - Validates against label_map\n    \"\"\"\n    # Clean text\n    cleaned = series.astype(str).str.strip().str.lower()\n\n    # Apply alias mapping\n    cleaned = cleaned.replace(aliases)\n\n    # Validate\n    bad_mask = ~cleaned.isin(label_map.keys())\n    if bad_mask.any():\n        bad_examples = series[bad_mask].unique()\n        raise ValueError(\n            f\"‚ùå Unmapped labels in '{colname}': {bad_examples}. \"\n            f\"Expected one of {list(label_map.keys())}\"\n        )\n\n    # Map to integers\n    return cleaned.map(label_map).to_numpy(dtype=np.int64)\n\ny_s = encode_series(df[\"sentiment\"], SENTIMENT_MAP, SENTIMENT_ALIASES, \"sentiment\")\ny_t = encode_series(df[\"truthfulness\"], TRUTH_MAP, TRUTH_ALIASES, \"truthfulness\")","metadata":{"_uuid":"11c10003-e4f5-4895-b3ff-cb7ac1b16c26","_cell_guid":"f618f42b-1a22-4c85-97f5-38b4ef5cd9f7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train/val/test split\ndf_train, df_temp, y_s_train, y_s_temp, y_t_train, y_t_temp = train_test_split(\n    df, y_s, y_t,\n    test_size=0.2,\n    random_state=SEED,\n    stratify=y_s\n)\ndf_val, df_test, y_s_val, y_s_test, y_t_val, y_t_test = train_test_split(\n    df_temp, y_s_temp, y_t_temp,\n    test_size=0.5,\n    random_state=SEED,\n    stratify=y_s_temp\n)\n\nprint(\"Train:\", df_train.shape, \"Val:\", df_val.shape, \"Test:\", df_test.shape)","metadata":{"_uuid":"f8de50be-f66a-4119-985e-4df35159f88c","_cell_guid":"a9771c6c-3551-4682-a6bd-20c9772bdb81","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Double-check directory exists before saving\nif not os.path.exists(SAVE_DIR):\n    os.makedirs(SAVE_DIR, exist_ok=True)\n    print(f\"Created directory: {SAVE_DIR}\")\n\n# Save label maps\nwith open(os.path.join(SAVE_DIR, \"sentiment_map.json\"), \"w\", encoding=\"utf-8\") as f:\n    json.dump(SENTIMENT_MAP, f, ensure_ascii=False, indent=2)\nwith open(os.path.join(SAVE_DIR, \"truth_map.json\"), \"w\", encoding=\"utf-8\") as f:\n    json.dump(TRUTH_MAP, f, ensure_ascii=False, indent=2)","metadata":{"_uuid":"0f322fa2-4a4f-462f-8827-a5b6c44ffa34","_cell_guid":"4b69b277-3ffe-4a5a-a5d7-b20d1f908880","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counts = np.bincount(y_s_train, minlength=3)\nfreqs = counts / counts.sum()\ninv = 1.0 / np.clip(freqs, 1e-8, None)\n\nsent_class_weights = torch.tensor(inv / inv.mean(), dtype=torch.float32, device=DEVICE)\nsent_class_weights = sent_class_weights.half()\nprint(\"Sentiment counts:\", counts, \" -> class weights:\", sent_class_weights.tolist())","metadata":{"_uuid":"69aca312-0240-4f80-976f-52484f5d205d","_cell_guid":"723e518c-9274-459c-8c8a-d8926650717f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counts_t = np.bincount(y_t_train, minlength=2)\nfreqs_t = counts_t / counts_t.sum()\ninv_t = 1.0 / np.clip(freqs_t, 1e-8, None)\ntruth_class_weights = torch.tensor(inv_t / inv_t.mean(), dtype=torch.float32, device=DEVICE)\n\n# ‚úÖ CAST TO FP16 TO MATCH QUANTIZED MODEL'S COMPUTATIONS\ntruth_class_weights = truth_class_weights.half()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"_uuid":"c73585cb-771d-4173-9578-ee8c0b300190","_cell_guid":"48b383ed-6232-44da-9185-2ffa09a09edc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_dataset(df, tokenizer, max_length=MAX_LENGTH, batch_size=512):\n    texts = df[\"text\"].astype(str).tolist()\n    all_input_ids, all_attention_mask = [], []\n    \n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n        batch = texts[i:i + batch_size]\n        encoded = tokenizer(batch, padding=\"max_length\", truncation=True, \n                           max_length=max_length, return_tensors=\"pt\")\n        all_input_ids.append(encoded[\"input_ids\"])\n        all_attention_mask.append(encoded[\"attention_mask\"])\n    \n    return {\n        \"input_ids\": torch.cat(all_input_ids, dim=0),\n        \"attention_mask\": torch.cat(all_attention_mask, dim=0)\n    }","metadata":{"_uuid":"8609f466-ce87-4eb4-b347-9c878097dbd9","_cell_guid":"5ea644e9-d8ba-4731-abf0-b43ba9901ad8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Caching\nCACHE_DIR = os.path.join(SAVE_DIR, \"tokenized_data\")\nos.makedirs(CACHE_DIR, exist_ok=True)\n\ndef load_or_tokenize(df, tokenizer, split_name):\n    cache_path = os.path.join(CACHE_DIR, f\"{split_name}_encodings.pt\")\n    if os.path.exists(cache_path):\n        print(f\"üîÅ Loading cached {split_name}...\")\n        return torch.load(cache_path)\n    print(f\"üìù Tokenizing {split_name}...\")\n    encodings = tokenize_dataset(df, tokenizer, MAX_LENGTH)\n    torch.save(encodings, cache_path)\n    return encodings","metadata":{"_uuid":"72bde69d-a686-4bf4-85f0-9d3bf0c76b23","_cell_guid":"7a2b529e-233e-49a8-9a43-edc3916cd6e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PreTokenizedDataset that stores the original dataframe indices\nclass PreTokenizedDataset(Dataset):\n    def __init__(self, encodings, y_s, y_t, orig_idx):\n        \"\"\"\n        encodings: dict of tensors 'input_ids','attention_mask' (shape N x L)\n        y_s, y_t: numpy arrays or lists (length N)\n        orig_idx: list/array of original row indices from the dataframe (length N)\n        \"\"\"\n        self.input_ids = encodings[\"input_ids\"]\n        self.attention_mask = encodings[\"attention_mask\"]\n        self.y_s = torch.tensor(y_s, dtype=torch.long)\n        self.y_t = torch.tensor(y_t, dtype=torch.long)\n        self.orig_idx = torch.tensor(orig_idx, dtype=torch.long)\n\n        # Sanity check\n        n = self.input_ids.shape[0]\n        assert self.attention_mask.shape[0] == n\n        assert len(self.y_s) == n\n        assert len(self.orig_idx) == n\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        # return orig_idx as Python int (or tensor) as well\n        return self.input_ids[idx], self.attention_mask[idx], self.y_s[idx], self.y_t[idx], int(self.orig_idx[idx])\n\n# Collator that preserves indices\n@dataclass\nclass Collator:\n    def __call__(self, batch):\n        # batch is list of tuples: (input_ids, attention_mask, y_s, y_t, orig_idx)\n        input_ids, attention_mask, ys, yt, idxs = zip(*batch)\n        return (torch.stack(input_ids),\n                torch.stack(attention_mask),\n                torch.tensor(ys, dtype=torch.long),\n                torch.tensor(yt, dtype=torch.long),\n                torch.tensor(idxs, dtype=torch.long))","metadata":{"_uuid":"10b7467a-8275-4259-82b0-d791fd1a52ee","_cell_guid":"561ef4b1-b6c9-4a22-be44-f4debaf3a5c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_encodings = load_or_tokenize(df_train, tokenizer, \"train\")\nval_encodings = load_or_tokenize(df_val, tokenizer, \"val\")\ntest_encodings = load_or_tokenize(df_test, tokenizer, \"test\")\n\ntrain_idx = df_train.index.tolist()\nval_idx   = df_val.index.tolist()\ntest_idx  = df_test.index.tolist()\n\ntrain_ds = PreTokenizedDataset(train_encodings, y_s_train, y_t_train, train_idx)\nval_ds   = PreTokenizedDataset(val_encodings,   y_s_val,   y_t_val,   val_idx)\ntest_ds  = PreTokenizedDataset(test_encodings,  y_s_test,  y_t_test,  test_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"collate_fn = Collator()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True, prefetch_factor=2)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)","metadata":{"_uuid":"fc835468-8295-4fca-b0cc-a56bce381b9e","_cell_guid":"b8f64e21-a38a-4e60-b145-51665ebd05a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\" * 80)\nprint(\"üîç DATASET LABEL SANITY CHECK - RUN BEFORE EVERY EXPERIMENT\")\nprint(\"=\" * 80)\n\ndef check_label_consistency(series, label_map, aliases, col_name, split_name):\n    \"\"\"\n    Check alignment between raw labels, normalized labels, and the target map.\n    Returns a DataFrame with samples for manual inspection.\n    \"\"\"\n    print(f\"\\n--- {split_name} Set: '{col_name}' ---\")\n\n    # Step 1: Get unique raw labels\n    raw_labels = series.astype(str).str.strip().unique()\n    print(f\"  Unique RAW labels ({len(raw_labels)}): {sorted(raw_labels)}\")\n\n    # Step 2: Apply alias normalization\n    cleaned = series.astype(str).str.strip().str.lower()\n    cleaned = cleaned.replace(aliases)\n    normalized_labels = cleaned.unique()\n    print(f\"  After ALIAS normalization ({len(normalized_labels)}): {sorted(normalized_labels)}\")\n\n    # Step 3: Check against final label map\n    unmapped = [lbl for lbl in normalized_labels if lbl not in label_map]\n    if unmapped:\n        print(f\"  ‚ùå CRITICAL: Unmapped labels found: {unmapped}\")\n    else:\n        print(f\"  ‚úÖ All normalized labels map to target IDs.\")\n\n    # Step 4: Show a sample mapping for clarity\n    sample_df = pd.DataFrame({\n        'raw_label': series.astype(str).str.strip().head(5).tolist(),\n        'normalized': cleaned.head(5).tolist(),\n        'mapped_id': cleaned.head(5).map(label_map).tolist()\n    })\n    print(f\"  Sample mapping (first 5 rows):\")\n    print(sample_df.to_string(index=False))\n\n    return unmapped\n\n# --- Run checks for Sentiment ---\nprint(\"\\n\" + \"=\"*40)\nprint(\"SENTIMENT LABELS\")\nprint(\"=\"*40)\n\nsent_train_unmapped = check_label_consistency(df_train[\"sentiment\"], SENTIMENT_MAP, SENTIMENT_ALIASES, \"sentiment\", \"TRAIN\")\nsent_val_unmapped = check_label_consistency(df_val[\"sentiment\"], SENTIMENT_MAP, SENTIMENT_ALIASES, \"sentiment\", \"VAL\")\nsent_test_unmapped = check_label_consistency(df_test[\"sentiment\"], SENTIMENT_MAP, SENTIMENT_ALIASES, \"sentiment\", \"TEST\")\n\n# --- Run checks for Truthfulness ---\nprint(\"\\n\" + \"=\"*40)\nprint(\"TRUTHFULNESS LABELS\")\nprint(\"=\"*40)\n\ntruth_train_unmapped = check_label_consistency(df_train[\"truthfulness\"], TRUTH_MAP, TRUTH_ALIASES, \"truthfulness\", \"TRAIN\")\ntruth_val_unmapped = check_label_consistency(df_val[\"truthfulness\"], TRUTH_MAP, TRUTH_ALIASES, \"truthfulness\", \"VAL\")\ntruth_test_unmapped = check_label_consistency(df_test[\"truthfulness\"], TRUTH_MAP, TRUTH_ALIASES, \"truthfulness\", \"TEST\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Final Summary ---\nprint(\"\\n\" + \"=\"*80)\nif (not sent_train_unmapped and not sent_val_unmapped and not sent_test_unmapped and\n    not truth_train_unmapped and not truth_val_unmapped and not truth_test_unmapped):\n    print(\"üéâ ALL CHECKS PASSED! Dataset labels are consistent and ready for training.\")\nelse:\n    print(\"üö® SANITY CHECK FAILED! Fix the unmapped labels above before proceeding.\")\n    raise ValueError(\"Label mapping inconsistency detected. Please fix before training.\")\n\nprint(\"=\" * 80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\nfocal_loss_fn = FocalLoss(alpha=1, gamma=2).to(DEVICE).half()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Model Definitions -----\nfrom transformers import ElectraConfig,BitsAndBytesConfig\n\nclass BanglaBERTConfig(ElectraConfig):\n    model_type = \"banglabert-multitask\"\n    def __init__(self, n_sentiment=3, n_truth=2, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.n_sentiment = n_sentiment\n        self.n_truth = n_truth\n        self.dropout = dropout","metadata":{"_uuid":"0a289933-a285-40a5-abe0-10b68c627145","_cell_guid":"69a4ef38-120f-4413-9852-f684f806a86a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BanglaBERTMultiTask(PreTrainedModel):\n    config_class = BanglaBERTConfig\n    def __init__(self, config, use_gradient_checkpointing=False):\n        super().__init__(config)\n        # Load the encoder WITHOUT quantization\n        self.encoder = AutoModel.from_pretrained(MODEL_NAME)  # ‚úÖ No quantization_config\n        if use_gradient_checkpointing:\n            self.encoder.gradient_checkpointing_enable()\n        self.dropout = nn.Dropout(config.dropout)\n        self.sent_head = nn.Linear(config.hidden_size, config.n_sentiment)\n        self.truth_head = nn.Linear(config.hidden_size, config.n_truth)\n    \n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls = self.dropout(outputs.last_hidden_state[:, 0])\n        s = self.sent_head(cls)\n        t = self.truth_head(cls)\n        return s, t","metadata":{"_uuid":"3f52310e-0535-4b40-b4b9-181e8feb3254","_cell_guid":"221fd55b-8495-4754-a270-4105e07e8c47","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(MODEL_NAME)\nconfig.update({\"n_sentiment\": 3, \"n_truth\": 2, \"dropout\": 0.5})\nmodel = BanglaBERTMultiTask(config, USE_GRAD_CHECKPOINT).to(DEVICE)","metadata":{"_uuid":"272f9f23-7c53-4b18-81ec-a5f57a312add","_cell_guid":"a8598705-5b3b-4f88-a27f-ba4460fd363c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_with_loss(model, loader, sent_loss_fn):\n    model.eval()\n    all_s, all_t = [], []\n    all_pred_s, all_pred_t = [], []\n    losses = []\n    orig_indices = []\n\n    \n    with torch.no_grad():  \n        for batch in loader:\n            input_ids, attn_mask, ys, yt, orig_idx = [b.to(DEVICE) for b in batch]\n\n            logits_s, logits_t = model(input_ids, attn_mask)\n\n            loss_s = sent_loss_fn(logits_s, ys)\n            loss_t = nn.CrossEntropyLoss(weight=truth_class_weights)(logits_t, yt)\n            loss = ALPHA_SENTIMENT * loss_s + (1 - ALPHA_SENTIMENT) * loss_t\n\n            losses.append(loss.item())\n\n            all_s.extend(ys.cpu().tolist())\n            all_t.extend(yt.cpu().tolist())\n            all_pred_s.extend(torch.argmax(logits_s, dim=1).cpu().tolist())\n            all_pred_t.extend(torch.argmax(logits_t, dim=1).cpu().tolist())\n            orig_indices.extend(orig_idx.cpu().tolist())\n\n    # Calculate core metrics\n    metrics = {\n        \"val_loss\": float(np.mean(losses)) if len(losses) > 0 else 0.0,\n        \"sentiment_acc\": accuracy_score(all_s, all_pred_s),\n        \"sentiment_f1\": f1_score(all_s, all_pred_s, average=\"macro\"),\n        \"truth_acc\": accuracy_score(all_t, all_pred_t),\n        \"truth_f1\": f1_score(all_t, all_pred_t, average=\"macro\"),\n        \"y_true_s\": all_s,\n        \"y_pred_s\": all_pred_s,\n        \"y_true_t\": all_t,\n        \"y_pred_t\": all_pred_t,\n        \"orig_indices\": orig_indices,\n    }\n\n    # Generate and add classification reports as strings\n    metrics[\"sentiment_report\"] = classification_report(\n        all_s,\n        all_pred_s,\n        target_names=[idx2sent[i] for i in range(len(SENTIMENT_MAP))],\n        output_dict=False\n    )\n    metrics[\"truth_report\"] = classification_report(\n        all_t,\n        all_pred_t,\n        target_names=[idx2truth[i] for i in range(len(TRUTH_MAP))],\n        output_dict=False\n    )\n\n    return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_with_loss(model, train_loader, val_loader, focal_loss_fn, lr, epochs=EPOCHS, save_dir=SAVE_DIR):\n    total_steps = len(train_loader) * epochs // GRAD_ACCUM_STEPS\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n    sched = get_cosine_schedule_with_warmup(opt, int(WARMUP_RATIO * total_steps), total_steps)\n\n    best_f1 = 0.0\n    patience, p_max = 2, 2\n    MIN_DELTA = 0.005\n\n    # Track losses and metrics\n    train_loss_history = []\n    val_loss_history = []\n    val_f1_history = []\n\n    print(f\"Starting training for {epochs} epochs...\")\n    for epoch in range(epochs):\n        model.train()\n        loss_running = 0.0\n        opt.zero_grad()\n\n        print(f\"Epoch {epoch+1}/{epochs} - Training:\", end=\"\", flush=True)\n        batch_count = 0\n\n        for step, batch in enumerate(train_loader):\n            input_ids, attn_mask, ys, yt, orig_idx = [b.to(DEVICE) for b in batch]\n            logits_s, logits_t = model(input_ids, attn_mask)\n            loss_s = focal_loss_fn(logits_s, ys)\n            loss_t = nn.CrossEntropyLoss(weight=truth_class_weights)(logits_t, yt)\n            loss = (ALPHA_SENTIMENT * loss_s + (1 - ALPHA_SENTIMENT) * loss_t) / GRAD_ACCUM_STEPS\n\n            loss.backward()\n            if torch.isnan(loss).any():\n                print(\"‚ö†Ô∏è  NaN loss detected!\")\n                for name, param in model.named_parameters():\n                    if param.grad is not None and torch.isnan(param.grad).any():\n                        print(f\"  NaN gradient in: {name}\")\n                        print(f\"  Param stats: mean={param.mean().item():.4f}, std={param.std().item():.4f}\")\n                        print(f\"  Grad stats: mean={param.grad.mean().item():.4f}, std={param.grad.std().item():.4f}\")\n                raise ValueError(\"Training aborted due to NaN loss.\")\n\n            loss_running += loss.item()\n\n            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n                opt.step()\n                opt.zero_grad()\n                sched.step()\n\n            batch_count += 1\n            if batch_count % 50 == 0:\n                print(\".\", end=\"\", flush=True)\n\n        print(\" Done\")\n        avg_train_loss = loss_running / len(train_loader)\n        train_loss_history.append(avg_train_loss)\n\n        # Clear memory after each epoch\n        torch.cuda.empty_cache()\n        gc.collect()\n\n        # Validation\n        print(f\"Epoch {epoch+1}/{epochs} - Validating:\", end=\"\", flush=True)\n        val_metrics = evaluate_with_loss(model, val_loader, focal_loss_fn) \n        val_loss_history.append(val_metrics[\"val_loss\"])\n        val_f1_history.append(val_metrics[\"sentiment_f1\"])\n        print(\"Done\")\n        print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {val_metrics['val_loss']:.4f}, F1: {val_metrics['sentiment_f1']:.3f}\")\n\n        # Check for overfitting\n        if epoch > 0 and val_loss_history[-1] > val_loss_history[-2] and train_loss_history[-1] < train_loss_history[-2]:\n            print(\"‚ö†Ô∏è  Warning: Possible overfitting detected!\")\n\n        improved = (val_metrics[\"sentiment_f1\"] - best_f1) > MIN_DELTA\n        if improved:\n            best_f1 = val_metrics[\"sentiment_f1\"]\n            patience = p_max\n            model.save_pretrained(save_dir)\n            tokenizer.save_pretrained(save_dir)\n            print(f\"üîß Saved new best model with F1: {best_f1:.4f}\")\n        else:\n            patience -= 1\n            print(f\"No sufficient improvement (delta <= {MIN_DELTA}); patience left: {patience}\")\n\n        if patience <= 0:\n            print(\"Early stopping triggered\")\n            break\n\n        print(\"-\" * 50)\n\n    # Plot training history (optional)\n    try:\n        plt.figure(figsize=(12, 4))\n        plt.subplot(1, 2, 1)\n        plt.plot(train_loss_history, label='Train Loss', marker='o')\n        plt.plot(val_loss_history, label='Val Loss', marker='s')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Training History')\n        plt.legend()\n        plt.grid(True)\n        plt.subplot(1, 2, 2)\n        plt.plot(val_f1_history, label='Val F1', color='green', marker='^')\n        plt.xlabel('Epoch')\n        plt.ylabel('F1 Score')\n        plt.title('Validation F1 Score')\n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        print(f\"Could not generate plots: {e}\")\n\n    print(\"Training loss history:\", train_loss_history)\n    print(\"Validation loss history:\", val_loss_history)\n    print(\"Validation F1 history:\", val_f1_history)\n\n    return val_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Unique sentiment labels:\", df[\"sentiment\"].unique())\nprint(\"Unique truth labels:\", df[\"truthfulness\"].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for NaN or Inf in your data\nfor batch in train_loader:\n    input_ids, attn_mask, ys, yt, orig_idx = [b.to(DEVICE) for b in batch]\n    if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():\n        print(\"‚ö†Ô∏è  NaN or Inf found in input_ids!\")\n        break\n    if torch.isnan(attn_mask).any() or torch.isinf(attn_mask).any():\n        print(\"‚ö†Ô∏è  NaN or Inf found in attn_mask!\")\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\n--- Training with LR={FIXED_LR}, FOCAL_LOSS, and ALPHA={ALPHA_SENTIMENT} for {EPOCHS} epochs ---\")\n\n# Clear cache before training\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Initialize model WITHOUT quantization\nfinal_model = BanglaBERTMultiTask(\n    config,\n    use_gradient_checkpointing=False\n).to(DEVICE)\n\n# Start training\ntrain_with_loss(\n    final_model,\n    train_loader,\n    val_loader,\n    focal_loss_fn,\n    lr=FIXED_LR,\n    epochs=EPOCHS\n)\n\n# Save model\nfinal_model.save_pretrained(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)","metadata":{"_uuid":"bd8e351d-bd58-4473-a4b5-75f14dfb2749","_cell_guid":"a1d05469-f329-46f6-b6d2-dc3289688c38","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_custom_model(model_path, device=DEVICE):\n    # Load the original config first\n    config = AutoConfig.from_pretrained(MODEL_NAME)\n    \n    # Add your custom parameters\n    config.n_sentiment = 3\n    config.n_truth = 2\n    config.dropout = 0.1\n    \n    # Create model\n    model = BanglaBERTMultiTask(config)\n    \n    # Find and load the model file (always load to CPU first)\n    model_files = [f for f in os.listdir(model_path) if f.endswith(('.bin', '.safetensors'))]\n    if not model_files:\n        raise FileNotFoundError(f\"No model file found in {model_path}\")\n    \n    model_file = os.path.join(model_path, model_files[0])\n    \n    if model_file.endswith(\".safetensors\"):\n        from safetensors.torch import load_file\n        state_dict = load_file(model_file, device=\"cpu\")\n    else:\n        state_dict = torch.load(model_file, map_location=\"cpu\")\n    \n    # Load weights (use strict=False to be more forgiving)\n    model.load_state_dict(state_dict, strict=False)\n    \n    # Move to target device\n    model.to(device)\n    model.eval()\n    \n    return model","metadata":{"_uuid":"1514ea68-2817-4962-9426-84553cd5d67e","_cell_guid":"64b00601-bb8d-4b8c-aab0-0836489e6ece","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_custom_model(SAVE_DIR)","metadata":{"_uuid":"f4288693-0b60-4189-8a91-f6236b42dde4","_cell_guid":"2adfe5e2-d7dd-4ca8-aa60-50429fd24edd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_metrics = evaluate_with_loss(model, test_loader, focal_loss_fn)\nprint(\"\\n=== Test Metrics ===\")\nprint(f\"Sentiment  : acc={test_metrics['sentiment_acc']:.3f}, macroF1={test_metrics['sentiment_f1']:.3f}\")\nprint(f\"Truthfulness: acc={test_metrics['truth_acc']:.3f}, macroF1={test_metrics['truth_f1']:.3f}\")\n\nprint(\"\\n=== Test Classification Reports ===\")\nprint(\"\\nSentiment Report:\")\nprint(test_metrics[\"sentiment_report\"])\nprint(\"\\nTruthfulness Report:\")\nprint(test_metrics[\"truth_report\"])","metadata":{"_uuid":"ceb648a3-eaed-4e11-a25c-73814bdeffda","_cell_guid":"41c0488d-1cbc-44cc-ad2d-da1527e7a275","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion matrices\ndef plot_cm(y_true, y_pred, labels, title):\n    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n    plt.figure(figsize=(4.5,4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(title)\n    plt.tight_layout(); plt.show()\n\nplot_cm(test_metrics[\"y_true_s\"], test_metrics[\"y_pred_s\"], [idx2sent[i] for i in range(3)], \"Sentiment (Test)\")\nplot_cm(test_metrics[\"y_true_t\"], test_metrics[\"y_pred_t\"], [idx2truth[i] for i in range(2)], \"Truthfulness (Test)\")","metadata":{"_uuid":"8f2884f1-fc8f-4fea-8849-390ef0c19324","_cell_guid":"a11ee5c8-f4f6-48cb-aff2-b39f00fe69d2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Error analysis dataframe\ndf_err = df_test.copy().reset_index(drop=True)\n\n# Add original raw labels explicitly\ndf_err[\"sentiment_raw\"] = df_test[\"sentiment\"].values\ndf_err[\"truthfulness_raw\"] = df_test[\"truthfulness\"].values\n\n# Add decoded (true) labels from eval\ndf_err[\"sent_true\"] = [idx2sent[i] for i in test_metrics[\"y_true_s\"]]\ndf_err[\"sent_pred\"] = [idx2sent[i] for i in test_metrics[\"y_pred_s\"]]\ndf_err[\"truth_true\"] = [idx2truth[i] for i in test_metrics[\"y_true_t\"]]\ndf_err[\"truth_pred\"] = [idx2truth[i] for i in test_metrics[\"y_pred_t\"]]\n\n# Correctness flags\ndf_err[\"sent_correct\"] = (df_err[\"sent_true\"] == df_err[\"sent_pred\"]).astype(int)\ndf_err[\"truth_correct\"] = (df_err[\"truth_true\"] == df_err[\"truth_pred\"]).astype(int)\ndf_err[\"both_correct\"] = (df_err[\"sent_correct\"].eq(1) & df_err[\"truth_correct\"].eq(1)).astype(int)\n\n# NEW: Mismatch flags (raw vs decoded)\ndf_err[\"sent_mismatch\"] = (df_err[\"sentiment_raw\"].str.lower() != df_err[\"sent_true\"].str.lower()).astype(int)\ndf_err[\"truth_mismatch\"] = (df_err[\"truthfulness_raw\"].str.lower() != df_err[\"truth_true\"].str.lower()).astype(int)","metadata":{"_uuid":"369cfb77-f9a7-416a-98fd-6c645287b47a","_cell_guid":"253a4b69-09bf-4f0a-977e-8370725c0b10","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cross_check_labels(df_raw, df_err, raw_col, decoded_col, label_map, aliases):\n    \"\"\"\n    Cross-check that raw labels match decoded labels after alias normalization.\n    \n    df_raw      : Original test DataFrame\n    df_err      : Error analysis DataFrame\n    raw_col     : Column in df_raw (e.g. 'sentiment' or 'truthfulness')\n    decoded_col : Column in df_err (e.g. 'sent_true' or 'truth_true')\n    label_map   : e.g. SENTIMENT_MAP or TRUTH_MAP\n    aliases     : e.g. SENTIMENT_ALIASES or TRUTH_ALIASES\n    \"\"\"\n    mismatches = []\n    for i, (raw, decoded) in enumerate(zip(df_raw[raw_col], df_err[decoded_col])):\n        raw_norm = aliases.get(str(raw).strip().lower(), str(raw).strip().lower())\n        if raw_norm not in label_map:\n            mismatches.append((i, raw, decoded, \"‚ùå unmapped\"))\n        else:\n            raw_std = [k for k,v in label_map.items() if k == raw_norm][0]\n            if raw_std != decoded:\n                mismatches.append((i, raw, decoded, \"‚ö† mismatch\"))\n    \n    if mismatches:\n        print(f\"‚ùå Found {len(mismatches)} mismatches in {raw_col}:\")\n        for m in mismatches[:10]:  # show first 10\n            print(m)\n    else:\n        print(f\"‚úÖ All {raw_col} values align with {decoded_col} after alias normalization.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cross_check_labels(df_test, df_err, \"sentiment\", \"sent_true\", SENTIMENT_MAP, SENTIMENT_ALIASES)\ncross_check_labels(df_test, df_err, \"truthfulness\", \"truth_true\", TRUTH_MAP, TRUTH_ALIASES)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"err_path = os.path.join(SAVE_DIR, \"error_analysis_test.xlsx\")\nwith pd.ExcelWriter(err_path, engine=\"openpyxl\") as writer:\n    df_err.to_excel(writer, index=False, sheet_name=\"errors\")\nprint(f\"‚úÖ Error analysis saved with raw + decoded labels, correctness, and mismatch flags to: {err_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"üìä ERROR ANALYSIS SUMMARY STATISTICS\")\nprint(\"=\"*60)\n\ntotal_rows = len(df_err)\nsent_mismatch_count = df_err['sent_mismatch'].sum()\ntruth_mismatch_count = df_err['truth_mismatch'].sum()\n\nprint(f\"Total rows in error analysis: {total_rows:,}\")\nprint(f\"Sentiment label mismatches (raw vs decoded): {sent_mismatch_count:,} ({sent_mismatch_count/total_rows:.2%})\")\nprint(f\"Truthfulness label mismatches (raw vs decoded): {truth_mismatch_count:,} ({truth_mismatch_count/total_rows:.2%})\")\n\nif sent_mismatch_count == 0 and truth_mismatch_count == 0:\n    print(\"\\n‚úÖ Perfect! No label mismatches found. Your data pipeline is solid.\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Label mismatches detected!\")\n    if sent_mismatch_count > 0:\n        print(f\"   - Investigate the 'sent_mismatch' column in the Excel file for sentiment issues.\")\n    if truth_mismatch_count > 0:\n        print(f\"   - Investigate the 'truth_mismatch' column in the Excel file for truthfulness issues.\")\n\nprint(\"\\nCorrectness Summary:\")\nsent_correct_count = df_err['sent_correct'].sum()\ntruth_correct_count = df_err['truth_correct'].sum()\nboth_correct_count = df_err['both_correct'].sum()\n\nprint(f\"Sentiment predictions correct: {sent_correct_count:,} ({sent_correct_count/total_rows:.2%})\")\nprint(f\"Truthfulness predictions correct: {truth_correct_count:,} ({truth_correct_count/total_rows:.2%})\")\nprint(f\"Both predictions correct: {both_correct_count:,} ({both_correct_count/total_rows:.2%})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final config\ninference_config = {\n    \"max_length\": MAX_LENGTH,\n    \"sentiment_map\": SENTIMENT_MAP,\n    \"truth_map\": TRUTH_MAP,\n    \"model_type\": \"banglabert-multitask\",\n    \"alpha_sentiment\": ALPHA_SENTIMENT\n}\nwith open(os.path.join(SAVE_DIR, \"inference_config.json\"), \"w\", encoding=\"utf-8\") as f:\n    json.dump(inference_config, f, ensure_ascii=False, indent=2)\n\nprint(\"‚úÖ All artifacts saved to:\", SAVE_DIR)","metadata":{"_uuid":"84afc0f1-e111-4ccb-aa71-1c236358f53f","_cell_guid":"a57216c6-0473-4e01-b570-d5977bb340f8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\ndemo_tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n\n# --- Define the prediction function ---\ndef predict_sentiment_and_truthfulness(text_input):\n    \"\"\"\n    Takes a raw Bengali text string and returns predicted sentiment and truthfulness.\n    \"\"\"\n    if not text_input or not text_input.strip():\n        return \"‚ö†Ô∏è Error: Please enter some Bengali text.\", \"‚ö†Ô∏è Error: Please enter some Bengali text.\"\n\n    try:\n        # Preprocess the text (mimic your training pipeline)\n        # 1. Apply full normalization (reusing your existing function)\n        text_clean = _clean_one(text_input)\n\n        # Tokenize\n        encoded = demo_tokenizer(\n            text_clean,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=MAX_LENGTH,\n            return_tensors=\"pt\"\n        )\n\n        # Move to device\n        input_ids = encoded[\"input_ids\"].to(DEVICE)\n        attention_mask = encoded[\"attention_mask\"].to(DEVICE)\n\n        # Predict\n        model.eval()\n        with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\"):\n            logits_s, logits_t = model(input_ids, attention_mask)\n\n        # Get predictions\n        pred_sent_idx = torch.argmax(logits_s, dim=1).item()\n        pred_truth_idx = torch.argmax(logits_t, dim=1).item()\n\n        pred_sent_label = idx2sent[pred_sent_idx]\n        pred_truth_label = idx2truth[pred_truth_idx]\n\n        # Format output for display (improved formatting)\n        sentiment_output = f\"**Predicted Sentiment:** `{pred_sent_label.upper()}`\"\n        truth_output = f\"**Predicted Truthfulness:** `{pred_truth_label.upper()}`\"\n\n        return sentiment_output, truth_output\n\n    except Exception as e:\n        error_msg = f\"‚ö†Ô∏è Prediction Error: {str(e)}\"\n        return error_msg, error_msg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Create the Gradio Interface ---\ndemo = gr.Interface(\n    fn=predict_sentiment_and_truthfulness,\n    inputs=gr.Textbox(\n        lines=5,\n        placeholder=\"Enter Bengali text here (e.g., '‡¶ï‡¶∞‡ßã‡¶®‡¶æ ‡¶≠‡¶æ‡¶á‡¶∞‡¶æ‡¶∏ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ ‡¶™‡¶¶‡¶ï‡ßç‡¶∑‡ßá‡¶™ ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶™‡ßç‡¶∞‡¶∂‡¶Ç‡¶∏‡¶®‡ßÄ‡¶Ø‡¶º‡•§')\",\n        label=\"üìù Enter Bengali Text\"\n    ),\n    outputs=[\n        gr.Markdown(label=\"üìä Predicted Sentiment\"),  # Using Markdown for better formatting\n        gr.Markdown(label=\"üîç Predicted Truthfulness\")\n    ],\n    title=\"üáßüá© Bengali Sentiment and Fake News Detector for COVID-19 Related Content\",\n    description=\"\"\"\n    **Model:** Fine-tuned BanglaBERT for Multi-Task Learning.\n    Enter a piece of Bengali text, click **Submit**, and get instant predictions for:\n    1. **Sentiment** (Positive, Negative, Neutral)\n    2. **Truthfulness** (Real, Fake)\n    \"\"\",\n    examples=[\n        [\"‡¶ï‡¶∞‡ßã‡¶®‡¶æ ‡¶≠‡¶æ‡¶á‡¶∞‡¶æ‡¶∏ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ ‡¶™‡¶¶‡¶ï‡ßç‡¶∑‡ßá‡¶™ ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶™‡ßç‡¶∞‡¶∂‡¶Ç‡¶∏‡¶®‡ßÄ‡¶Ø‡¶º‡•§\"],\n        [\"‡¶è‡¶á ‡¶≠‡ßç‡¶Ø‡¶æ‡¶ï‡¶∏‡¶ø‡¶®‡ßá ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶Æ‡¶æ‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá, ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶≤‡ßÅ‡¶ï‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶§‡¶•‡ßç‡¶Ø‡•§\"],\n        [\"‡¶≤‡¶ï‡¶°‡¶æ‡¶â‡¶®‡ßá‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£‡ßá ‡¶Ö‡¶®‡ßá‡¶ï‡ßá‡¶∞ ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶ö‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡ßá, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶è‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®‡ßÄ‡¶Ø‡¶º ‡¶õ‡¶ø‡¶≤‡•§\"]\n    ],\n    theme=\"soft\",\n    live=False,\n    submit_btn=\"üöÄ Get Prediction\",  \n    clear_btn=\"üóëÔ∏è Clear\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Launch the demo ---\nprint(\"üöÄ Launching Gradio demo...\")\ndemo.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}